{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511749dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tweets: ['love movie smilingfacewithsmilingeyes', 'terrible movie', 'watched new movie moviecamera']\n",
      "Padded Sequences: [[3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [6 7 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shrikantvarma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import emoji\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample data\n",
    "tweets = [\n",
    "    \"I love this movie! #awesome ðŸ˜Š\",\n",
    "    \"This is a terrible movie... :( http://example.com\",\n",
    "    \"Just watched the new movie @user ðŸŽ¥\"\n",
    "]\n",
    "\n",
    "# Function to convert emojis to text\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_tweet(tweet):\n",
    "    # Convert emojis to text\n",
    "    tweet = convert_emojis(tweet)\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    # Remove mentions\n",
    "    tweet = re.sub(r\"@\\S+\", \"\", tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r\"#\\S+\", \"\", tweet)\n",
    "    # Remove special characters and numbers\n",
    "    tweet = re.sub(r\"[^A-Za-z\\s]\", \"\", tweet)\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Tokenize and remove stopwords\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess tweets\n",
    "cleaned_tweets = [preprocess_tweet(tweet) for tweet in tweets]\n",
    "\n",
    "# Tokenization\n",
    "vocab_size = 5000\n",
    "max_length = 50\n",
    "embedding_dim = 100\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(cleaned_tweets)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_tweets)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Display preprocessed tweets\n",
    "print(\"Cleaned Tweets:\", cleaned_tweets)\n",
    "print(\"Padded Sequences:\", padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7866b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shrikantvarma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 128)         84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                1560      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,127,281\n",
      "Trainable params: 1,127,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 14:13:04.683854: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4971s 497ms/step - loss: 0.4732 - accuracy: 0.7788 - val_loss: 0.4439 - val_accuracy: 0.7910\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 4891s 489ms/step - loss: 0.4461 - accuracy: 0.7953 - val_loss: 0.4372 - val_accuracy: 0.7944\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 4940s 494ms/step - loss: 0.4339 - accuracy: 0.8021 - val_loss: 0.4340 - val_accuracy: 0.7972\n",
      "10000/10000 - 265s - loss: 0.4340 - accuracy: 0.7972 - 265s/epoch - 26ms/step\n",
      "Accuracy: 0.7972375154495239\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "Predictions: [[0.97376347]\n",
      " [0.02353369]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Select necessary columns\n",
    "df = df[['text', 'target']]\n",
    "\n",
    "# Convert target labels to binary (0 = negative, 4 = positive)\n",
    "df['target'] = df['target'].replace(4, 1)\n",
    "\n",
    "# Function to convert emojis to text\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_tweet(tweet):\n",
    "    # Convert emojis to text\n",
    "    tweet = convert_emojis(tweet)\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    # Remove mentions\n",
    "    tweet = re.sub(r\"@\\S+\", \"\", tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r\"#\\S+\", \"\", tweet)\n",
    "    # Remove special characters and numbers\n",
    "    tweet = re.sub(r\"[^A-Za-z\\s:]\", \"\", tweet)\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Tokenize and remove stopwords\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess tweets\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Tokenization\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "embedding_dim = 100\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_test, y_test), batch_size=128, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Predict on new data\n",
    "sample_tweets = [\"I love this product!\", \"This is the worst service ever.\", \"The new functionality to search is pretty cool\"]\n",
    "sample_sequences = tokenizer.texts_to_sequences([preprocess_tweet(tweet) for tweet in sample_tweets])\n",
    "sample_padded = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(sample_padded)\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a0dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_analysis_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8206c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predictions: [[0.97376347]\n",
      " [0.02353368]\n",
      " [0.9787137 ]\n",
      " [0.10517763]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on new data\n",
    "sample_tweets = [\"I love this product!\", \"This is the worst service ever.\", \"The new functionality to search is pretty cool\",\"I hate it\"]\n",
    "sample_sequences = tokenizer.texts_to_sequences([preprocess_tweet(tweet) for tweet in sample_tweets])\n",
    "sample_padded = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(sample_padded)\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7c870ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predictions: [[0.62145734]\n",
      " [0.83895487]\n",
      " [0.0557374 ]\n",
      " [0.4164418 ]\n",
      " [0.78586394]\n",
      " [0.90520096]\n",
      " [0.90960026]\n",
      " [0.03631585]\n",
      " [0.8024434 ]\n",
      " [0.35514972]\n",
      " [0.5321297 ]\n",
      " [0.72439975]\n",
      " [0.917778  ]\n",
      " [0.5559894 ]\n",
      " [0.49044085]\n",
      " [0.22875585]\n",
      " [0.7758942 ]]\n"
     ]
    }
   ],
   "source": [
    "sample_tweets = [\"The movie was very touching and heart whelming\", \n",
    "            \"I have never seen an awesome movie like this\", \n",
    "            \"the movie plot was great but it had terrible acting\",\n",
    "           \"I thought the movie would be boring\",\n",
    "           \"I thought the movie would be boring, but it was surprisingly good!\",\n",
    "               \"The movie was an absolute masterpiece, with stunning visuals and a compelling storyline.\",\n",
    "    \"I loved every minute of the film; the acting was top-notch, and the soundtrack was beautiful.\",\n",
    "    \"The plot was incredibly dull, and the characters were poorly developed. A complete waste of time.\",\n",
    "    \"I found the movie to be a predictable and uninspired rehash of better films.\",\n",
    "    \"The movie had an interesting concept, but the execution fell flat, and the pacing was all over the place.\",\n",
    "    \"While the cinematography was breathtaking, the plot was convoluted and hard to follow.\",\n",
    "    \"This film exceeded all my expectations with its brilliant script and stellar performances. A must-watch!\",\n",
    "    \"An emotionally gripping story that kept me on the edge of my seat. Highly recommend it.\",\n",
    "    \"The special effects were outdated, and the dialogue was cringeworthy. I couldn't wait for it to end.\",\n",
    "    \"Despite a few decent scenes, the overall experience was boring and forgettable.\",\n",
    "    \"The first half of the movie was engaging, but it lost momentum in the second half and ended on a weak note.\",\n",
    "    \"I appreciated the artistic direction, but the acting was subpar, and the story lacked coherence.\"\n",
    "]\n",
    "sample_sequences = tokenizer.texts_to_sequences([preprocess_tweet(tweet) for tweet in sample_tweets])\n",
    "sample_padded = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(sample_padded)\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c3c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
